Fri Dec 14 17:30:03 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |
| N/A   33C    P0    65W / 149W |      0MiB / 11441MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 00000000:05:00.0 Off |                    0 |
| N/A   43C    P0    75W / 149W |      0MiB / 11441MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K80           Off  | 00000000:84:00.0 Off |                    0 |
| N/A   34C    P0    66W / 149W |      0MiB / 11441MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla K80           Off  | 00000000:85:00.0 Off |                    0 |
| N/A   43C    P0    76W / 149W |      0MiB / 11441MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
/home/samwiq/ABC and deep learning project/abc-dl/lunarc
/home/samwiq/ABC and deep learning project/abc-dl
start script
500
check version of Knet
    Status `~/.julia/environments/v1.0/Project.toml`
  [336ed68f] CSV v0.3.1
  [a93c6f00] DataFrames v0.13.1
  [31c24e10] Distributions v0.16.4
  [1902f260] Knet v1.1.0
  [6f286f6a] MultivariateStats v0.6.0 #master (https://github.com/JuliaStats/MultivariateStats.jl.git)
  [2913bbd2] StatsBase v0.25.0
  [4c63d2b9] StatsFuns v0.7.0
build Knet
  Building SpecialFunctions → `~/.julia/packages/SpecialFunctions/KvXoO/deps/build.log`
  Building CodecZlib ───────→ `~/.julia/packages/CodecZlib/wwgbh/deps/build.log`
  Building Knet ────────────→ `~/.julia/packages/Knet/hxjeS/deps/build.log`
test gpu
1
0
0
Loading alpha-stable model
0
Nbr training obs 500000, nbr parameters 22214, obs/parameters 22.51
Starting training
Epoch 1, current loss (training) 0.5433, current loss (val) 0.5525, best loss (val) 0.5525 
Epoch 2, current loss (training) 0.3988, current loss (val) 0.4854, best loss (val) 0.4854 
Epoch 3, current loss (training) 1.5533, current loss (val) 0.6088, best loss (val) 0.4854 
Epoch 4, current loss (training) 0.3887, current loss (val) 0.4379, best loss (val) 0.4379 
Epoch 5, current loss (training) 0.3416, current loss (val) 0.3640, best loss (val) 0.3640 
Epoch 6, current loss (training) 0.2774, current loss (val) 0.3634, best loss (val) 0.3634 
Epoch 7, current loss (training) 0.2833, current loss (val) 0.3542, best loss (val) 0.3542 
Epoch 8, current loss (training) 0.5465, current loss (val) 0.6579, best loss (val) 0.3542 
Epoch 9, current loss (training) 0.9581, current loss (val) 0.3666, best loss (val) 0.3542 
Epoch 10, current loss (training) 0.1897, current loss (val) 0.3628, best loss (val) 0.3542 
Epoch 11, current loss (training) 0.2877, current loss (val) 0.4358, best loss (val) 0.3542 
Epoch 12, current loss (training) 0.1828, current loss (val) 0.3030, best loss (val) 0.3030 
Epoch 13, current loss (training) 0.2136, current loss (val) 0.3311, best loss (val) 0.3030 
Epoch 14, current loss (training) 0.5434, current loss (val) 0.3442, best loss (val) 0.3030 
Epoch 15, current loss (training) 0.2098, current loss (val) 0.3053, best loss (val) 0.3030 
Epoch 16, current loss (training) 0.1618, current loss (val) 0.2928, best loss (val) 0.2928 
Epoch 17, current loss (training) 0.5037, current loss (val) 0.3405, best loss (val) 0.2928 
Epoch 18, current loss (training) 0.3422, current loss (val) 0.3419, best loss (val) 0.2928 
Epoch 19, current loss (training) 0.3010, current loss (val) 0.2879, best loss (val) 0.2879 
Epoch 20, current loss (training) 0.5991, current loss (val) 0.4911, best loss (val) 0.2879 
Epoch 21, current loss (training) 0.2274, current loss (val) 0.2943, best loss (val) 0.2879 
Epoch 22, current loss (training) 0.5878, current loss (val) 0.3595, best loss (val) 0.2879 
Epoch 23, current loss (training) 0.2408, current loss (val) 0.3294, best loss (val) 0.2879 
Epoch 24, current loss (training) 0.1765, current loss (val) 0.2764, best loss (val) 0.2764 
Epoch 25, current loss (training) 0.6124, current loss (val) 0.2864, best loss (val) 0.2764 
Epoch 26, current loss (training) 0.3903, current loss (val) 0.3110, best loss (val) 0.2764 
Epoch 27, current loss (training) 0.3875, current loss (val) 0.3541, best loss (val) 0.2764 
Epoch 28, current loss (training) 0.4100, current loss (val) 0.3354, best loss (val) 0.2764 
Epoch 29, current loss (training) 0.4158, current loss (val) 0.3809, best loss (val) 0.2764 
Epoch 30, current loss (training) 0.7058, current loss (val) 0.2632, best loss (val) 0.2632 
Epoch 31, current loss (training) 0.2699, current loss (val) 0.3361, best loss (val) 0.2632 
Epoch 32, current loss (training) 0.3683, current loss (val) 0.2968, best loss (val) 0.2632 
Epoch 33, current loss (training) 0.2601, current loss (val) 0.3302, best loss (val) 0.2632 
Epoch 34, current loss (training) 0.2069, current loss (val) 0.2796, best loss (val) 0.2632 
Epoch 35, current loss (training) 0.2185, current loss (val) 0.3092, best loss (val) 0.2632 
Epoch 36, current loss (training) 0.3238, current loss (val) 0.2751, best loss (val) 0.2632 
Epoch 37, current loss (training) 0.4085, current loss (val) 0.4103, best loss (val) 0.2632 
Epoch 38, current loss (training) 0.3814, current loss (val) 0.3383, best loss (val) 0.2632 
Epoch 39, current loss (training) 0.2385, current loss (val) 0.3226, best loss (val) 0.2632 
Epoch 40, current loss (training) 0.9346, current loss (val) 0.3002, best loss (val) 0.2632 
Epoch 41, current loss (training) 0.3283, current loss (val) 0.2916, best loss (val) 0.2632 
Epoch 42, current loss (training) 0.1837, current loss (val) 0.2840, best loss (val) 0.2632 
Epoch 43, current loss (training) 0.3016, current loss (val) 0.2908, best loss (val) 0.2632 
Epoch 44, current loss (training) 1.1593, current loss (val) 0.2856, best loss (val) 0.2632 
Epoch 45, current loss (training) 0.1547, current loss (val) 0.2758, best loss (val) 0.2632 
Epoch 46, current loss (training) 0.3514, current loss (val) 0.3243, best loss (val) 0.2632 
Epoch 47, current loss (training) 0.5603, current loss (val) 0.2901, best loss (val) 0.2632 
Epoch 48, current loss (training) 0.2288, current loss (val) 0.2808, best loss (val) 0.2632 
Epoch 49, current loss (training) 0.1997, current loss (val) 0.2923, best loss (val) 0.2632 
Epoch 50, current loss (training) 0.2977, current loss (val) 0.2682, best loss (val) 0.2632 
Epoch 51, current loss (training) 0.2109, current loss (val) 0.3091, best loss (val) 0.2632 
Epoch 52, current loss (training) 0.4935, current loss (val) 0.3716, best loss (val) 0.2632 
Epoch 53, current loss (training) 0.5568, current loss (val) 0.2594, best loss (val) 0.2594 
Epoch 54, current loss (training) 0.3592, current loss (val) 0.2984, best loss (val) 0.2594 
Epoch 55, current loss (training) 0.2525, current loss (val) 0.2760, best loss (val) 0.2594 
Epoch 56, current loss (training) 0.1610, current loss (val) 0.2626, best loss (val) 0.2594 
Epoch 57, current loss (training) 0.1828, current loss (val) 0.3272, best loss (val) 0.2594 
Epoch 58, current loss (training) 0.1824, current loss (val) 0.3208, best loss (val) 0.2594 
Epoch 59, current loss (training) 0.2160, current loss (val) 0.2794, best loss (val) 0.2594 
Epoch 60, current loss (training) 0.3595, current loss (val) 0.2723, best loss (val) 0.2594 
Epoch 61, current loss (training) 0.2251, current loss (val) 0.2530, best loss (val) 0.2530 
Epoch 62, current loss (training) 0.1679, current loss (val) 0.2660, best loss (val) 0.2530 
Epoch 63, current loss (training) 0.2811, current loss (val) 0.2707, best loss (val) 0.2530 
Epoch 64, current loss (training) 0.1570, current loss (val) 0.2755, best loss (val) 0.2530 
Epoch 65, current loss (training) 0.1783, current loss (val) 0.2680, best loss (val) 0.2530 
Epoch 66, current loss (training) 0.3219, current loss (val) 0.3210, best loss (val) 0.2530 
Epoch 67, current loss (training) 0.1441, current loss (val) 0.3101, best loss (val) 0.2530 
Epoch 68, current loss (training) 0.5199, current loss (val) 0.3661, best loss (val) 0.2530 
Epoch 69, current loss (training) 0.1646, current loss (val) 0.2854, best loss (val) 0.2530 
Epoch 70, current loss (training) 0.7383, current loss (val) 0.3067, best loss (val) 0.2530 
Epoch 71, current loss (training) 0.3305, current loss (val) 0.3195, best loss (val) 0.2530 
Epoch 72, current loss (training) 0.1677, current loss (val) 0.2739, best loss (val) 0.2530 
Epoch 73, current loss (training) 0.1832, current loss (val) 0.2696, best loss (val) 0.2530 
Epoch 74, current loss (training) 0.2959, current loss (val) 0.2816, best loss (val) 0.2530 
Epoch 75, current loss (training) 0.2113, current loss (val) 0.2593, best loss (val) 0.2530 
Epoch 76, current loss (training) 0.3391, current loss (val) 0.2949, best loss (val) 0.2530 
Epoch 77, current loss (training) 0.2088, current loss (val) 0.2893, best loss (val) 0.2530 
Epoch 78, current loss (training) 0.1685, current loss (val) 0.2729, best loss (val) 0.2530 
Epoch 79, current loss (training) 0.2781, current loss (val) 0.2924, best loss (val) 0.2530 
Epoch 80, current loss (training) 0.2393, current loss (val) 0.2718, best loss (val) 0.2530 
Epoch 81, current loss (training) 0.2224, current loss (val) 0.2948, best loss (val) 0.2530 
Epoch 82, current loss (training) 0.3119, current loss (val) 0.2637, best loss (val) 0.2530 
Epoch 83, current loss (training) 0.1689, current loss (val) 0.3046, best loss (val) 0.2530 
Epoch 84, current loss (training) 0.1794, current loss (val) 0.2848, best loss (val) 0.2530 
Epoch 85, current loss (training) 0.1950, current loss (val) 0.2696, best loss (val) 0.2530 
Epoch 86, current loss (training) 0.1738, current loss (val) 0.3080, best loss (val) 0.2530 
Epoch 87, current loss (training) 0.6178, current loss (val) 0.5642, best loss (val) 0.2530 
Epoch 88, current loss (training) 0.3622, current loss (val) 0.2624, best loss (val) 0.2530 
Epoch 89, current loss (training) 0.1856, current loss (val) 0.2675, best loss (val) 0.2530 
Epoch 90, current loss (training) 0.3224, current loss (val) 0.2810, best loss (val) 0.2530 
Epoch 91, current loss (training) 0.1659, current loss (val) 0.2741, best loss (val) 0.2530 
Epoch 92, current loss (training) 0.2482, current loss (val) 0.2777, best loss (val) 0.2530 
Epoch 93, current loss (training) 0.1562, current loss (val) 0.2700, best loss (val) 0.2530 
Epoch 94, current loss (training) 0.3208, current loss (val) 0.2835, best loss (val) 0.2530 
Epoch 95, current loss (training) 0.2249, current loss (val) 0.2566, best loss (val) 0.2530 
Epoch 96, current loss (training) 0.1970, current loss (val) 0.2488, best loss (val) 0.2488 
Epoch 97, current loss (training) 0.6017, current loss (val) 0.2853, best loss (val) 0.2488 
Epoch 98, current loss (training) 0.1660, current loss (val) 0.2586, best loss (val) 0.2488 
Epoch 99, current loss (training) 0.1562, current loss (val) 0.2648, best loss (val) 0.2488 
Epoch 100, current loss (training) 0.1815, current loss (val) 0.3192, best loss (val) 0.2488 
Epoch 101, current loss (training) 0.1982, current loss (val) 0.2734, best loss (val) 0.2488 
Epoch 102, current loss (training) 0.6543, current loss (val) 0.2904, best loss (val) 0.2488 
Epoch 103, current loss (training) 0.2471, current loss (val) 0.2629, best loss (val) 0.2488 
Epoch 104, current loss (training) 0.1470, current loss (val) 0.2550, best loss (val) 0.2488 
Epoch 105, current loss (training) 0.1866, current loss (val) 0.2714, best loss (val) 0.2488 
Epoch 106, current loss (training) 0.1371, current loss (val) 0.2840, best loss (val) 0.2488 
Epoch 107, current loss (training) 0.1891, current loss (val) 0.2637, best loss (val) 0.2488 
Epoch 108, current loss (training) 0.1971, current loss (val) 0.2847, best loss (val) 0.2488 
Epoch 109, current loss (training) 0.1665, current loss (val) 0.2505, best loss (val) 0.2488 
Epoch 110, current loss (training) 0.1596, current loss (val) 0.2940, best loss (val) 0.2488 
Epoch 111, current loss (training) 0.5031, current loss (val) 0.2692, best loss (val) 0.2488 
Epoch 112, current loss (training) 0.4463, current loss (val) 0.4725, best loss (val) 0.2488 
Epoch 113, current loss (training) 0.1909, current loss (val) 0.2878, best loss (val) 0.2488 
Epoch 114, current loss (training) 0.1638, current loss (val) 0.2846, best loss (val) 0.2488 
Epoch 115, current loss (training) 0.1903, current loss (val) 0.2554, best loss (val) 0.2488 
Epoch 116, current loss (training) 0.1743, current loss (val) 0.2554, best loss (val) 0.2488 
Epoch 117, current loss (training) 0.5590, current loss (val) 0.2641, best loss (val) 0.2488 
Epoch 118, current loss (training) 0.2637, current loss (val) 0.2784, best loss (val) 0.2488 
Epoch 119, current loss (training) 0.2945, current loss (val) 0.3709, best loss (val) 0.2488 
Epoch 120, current loss (training) 0.1563, current loss (val) 0.2555, best loss (val) 0.2488 
Epoch 121, current loss (training) 0.1719, current loss (val) 0.2928, best loss (val) 0.2488 
Epoch 122, current loss (training) 0.2499, current loss (val) 0.2843, best loss (val) 0.2488 
Epoch 123, current loss (training) 0.3622, current loss (val) 0.3057, best loss (val) 0.2488 
Epoch 124, current loss (training) 0.2549, current loss (val) 0.2816, best loss (val) 0.2488 
Epoch 125, current loss (training) 0.3522, current loss (val) 0.2778, best loss (val) 0.2488 
Epoch 126, current loss (training) 0.1912, current loss (val) 0.2563, best loss (val) 0.2488 
Epoch 127, current loss (training) 0.4465, current loss (val) 0.2727, best loss (val) 0.2488 
Epoch 128, current loss (training) 0.2226, current loss (val) 0.2908, best loss (val) 0.2488 
Epoch 129, current loss (training) 0.2952, current loss (val) 0.2456, best loss (val) 0.2456 
Epoch 130, current loss (training) 0.1756, current loss (val) 0.2667, best loss (val) 0.2456 
Epoch 131, current loss (training) 0.1660, current loss (val) 0.2818, best loss (val) 0.2456 
Epoch 132, current loss (training) 0.2541, current loss (val) 0.3193, best loss (val) 0.2456 
Epoch 133, current loss (training) 0.1757, current loss (val) 0.2826, best loss (val) 0.2456 
Epoch 134, current loss (training) 0.2809, current loss (val) 0.2958, best loss (val) 0.2456 
Epoch 135, current loss (training) 0.1671, current loss (val) 0.2805, best loss (val) 0.2456 
Epoch 136, current loss (training) 0.2252, current loss (val) 0.3673, best loss (val) 0.2456 
Epoch 137, current loss (training) 0.2148, current loss (val) 0.2803, best loss (val) 0.2456 
Epoch 138, current loss (training) 0.3289, current loss (val) 0.3477, best loss (val) 0.2456 
Epoch 139, current loss (training) 0.1726, current loss (val) 0.2507, best loss (val) 0.2456 
Epoch 140, current loss (training) 0.1613, current loss (val) 0.2470, best loss (val) 0.2456 
Epoch 141, current loss (training) 0.3088, current loss (val) 0.3467, best loss (val) 0.2456 
Epoch 142, current loss (training) 0.2120, current loss (val) 0.2683, best loss (val) 0.2456 
Epoch 143, current loss (training) 0.2272, current loss (val) 0.2380, best loss (val) 0.2380 
Epoch 144, current loss (training) 0.4934, current loss (val) 0.2913, best loss (val) 0.2380 
Epoch 145, current loss (training) 0.3660, current loss (val) 0.2611, best loss (val) 0.2380 
Epoch 146, current loss (training) 0.3114, current loss (val) 0.2579, best loss (val) 0.2380 
Epoch 147, current loss (training) 0.3544, current loss (val) 0.2974, best loss (val) 0.2380 
Epoch 148, current loss (training) 0.1918, current loss (val) 0.3048, best loss (val) 0.2380 
Epoch 149, current loss (training) 0.1853, current loss (val) 0.2885, best loss (val) 0.2380 
Epoch 150, current loss (training) 0.2424, current loss (val) 0.2549, best loss (val) 0.2380 
Epoch 151, current loss (training) 0.1848, current loss (val) 0.2930, best loss (val) 0.2380 
Epoch 152, current loss (training) 0.2142, current loss (val) 0.2699, best loss (val) 0.2380 
Epoch 153, current loss (training) 0.2028, current loss (val) 0.3018, best loss (val) 0.2380 
Epoch 154, current loss (training) 0.2075, current loss (val) 0.2800, best loss (val) 0.2380 
Epoch 155, current loss (training) 0.1705, current loss (val) 0.2666, best loss (val) 0.2380 
Epoch 156, current loss (training) 0.1790, current loss (val) 0.2842, best loss (val) 0.2380 
Epoch 157, current loss (training) 0.3099, current loss (val) 0.3388, best loss (val) 0.2380 
Epoch 158, current loss (training) 0.1855, current loss (val) 0.2619, best loss (val) 0.2380 
Epoch 159, current loss (training) 0.1679, current loss (val) 0.3033, best loss (val) 0.2380 
Epoch 160, current loss (training) 0.2143, current loss (val) 0.2438, best loss (val) 0.2380 
Epoch 161, current loss (training) 0.1925, current loss (val) 0.2667, best loss (val) 0.2380 
Epoch 162, current loss (training) 0.2890, current loss (val) 0.2664, best loss (val) 0.2380 
Epoch 163, current loss (training) 0.2313, current loss (val) 0.2857, best loss (val) 0.2380 
Epoch 164, current loss (training) 0.2230, current loss (val) 0.2510, best loss (val) 0.2380 
Epoch 165, current loss (training) 0.1762, current loss (val) 0.2601, best loss (val) 0.2380 
Epoch 166, current loss (training) 0.9937, current loss (val) 0.2608, best loss (val) 0.2380 
Epoch 167, current loss (training) 0.1973, current loss (val) 0.2517, best loss (val) 0.2380 
Epoch 168, current loss (training) 0.1636, current loss (val) 0.3112, best loss (val) 0.2380 
Epoch 169, current loss (training) 0.1824, current loss (val) 0.2639, best loss (val) 0.2380 
Epoch 170, current loss (training) 0.2257, current loss (val) 0.2625, best loss (val) 0.2380 
Epoch 171, current loss (training) 0.2287, current loss (val) 0.2597, best loss (val) 0.2380 
Epoch 172, current loss (training) 0.1885, current loss (val) 0.2799, best loss (val) 0.2380 
Epoch 173, current loss (training) 0.6426, current loss (val) 0.2551, best loss (val) 0.2380 
Epoch 174, current loss (training) 0.1629, current loss (val) 0.3051, best loss (val) 0.2380 
Epoch 175, current loss (training) 0.1754, current loss (val) 0.2843, best loss (val) 0.2380 
Epoch 176, current loss (training) 0.3314, current loss (val) 0.2632, best loss (val) 0.2380 
Epoch 177, current loss (training) 0.4465, current loss (val) 0.2517, best loss (val) 0.2380 
Epoch 178, current loss (training) 0.3008, current loss (val) 0.2890, best loss (val) 0.2380 
Epoch 179, current loss (training) 0.1630, current loss (val) 0.2834, best loss (val) 0.2380 
Epoch 180, current loss (training) 0.1829, current loss (val) 0.2651, best loss (val) 0.2380 
Epoch 181, current loss (training) 0.2021, current loss (val) 0.2949, best loss (val) 0.2380 
Epoch 182, current loss (training) 0.2077, current loss (val) 0.2572, best loss (val) 0.2380 
Epoch 183, current loss (training) 0.2611, current loss (val) 0.2925, best loss (val) 0.2380 
Epoch 184, current loss (training) 0.1850, current loss (val) 0.2944, best loss (val) 0.2380 
Epoch 185, current loss (training) 0.1770, current loss (val) 0.2525, best loss (val) 0.2380 
Epoch 186, current loss (training) 0.6227, current loss (val) 0.2352, best loss (val) 0.2352 
Epoch 187, current loss (training) 0.4312, current loss (val) 0.3428, best loss (val) 0.2352 
Epoch 188, current loss (training) 0.2033, current loss (val) 0.2801, best loss (val) 0.2352 
Epoch 189, current loss (training) 0.2266, current loss (val) 0.2796, best loss (val) 0.2352 
Epoch 190, current loss (training) 0.1987, current loss (val) 0.2613, best loss (val) 0.2352 
Epoch 191, current loss (training) 0.1857, current loss (val) 0.2755, best loss (val) 0.2352 
Epoch 192, current loss (training) 0.2678, current loss (val) 0.3183, best loss (val) 0.2352 
Epoch 193, current loss (training) 0.2299, current loss (val) 0.2576, best loss (val) 0.2352 
Epoch 194, current loss (training) 0.2139, current loss (val) 0.2779, best loss (val) 0.2352 
Epoch 195, current loss (training) 0.2063, current loss (val) 0.3853, best loss (val) 0.2352 
Epoch 196, current loss (training) 0.1416, current loss (val) 0.2626, best loss (val) 0.2352 
Epoch 197, current loss (training) 0.6382, current loss (val) 0.3081, best loss (val) 0.2352 
Epoch 198, current loss (training) 0.3366, current loss (val) 0.3054, best loss (val) 0.2352 
Epoch 199, current loss (training) 0.2660, current loss (val) 0.2788, best loss (val) 0.2352 
Epoch 200, current loss (training) 0.4930, current loss (val) 0.2706, best loss (val) 0.2352 
Epoch 201, current loss (training) 0.2719, current loss (val) 0.2824, best loss (val) 0.2352 
Epoch 202, current loss (training) 0.1784, current loss (val) 0.2412, best loss (val) 0.2352 
Epoch 203, current loss (training) 0.2835, current loss (val) 0.2623, best loss (val) 0.2352 
Epoch 204, current loss (training) 0.1909, current loss (val) 0.2607, best loss (val) 0.2352 
Epoch 205, current loss (training) 0.3752, current loss (val) 0.2462, best loss (val) 0.2352 
Epoch 206, current loss (training) 0.3672, current loss (val) 0.2688, best loss (val) 0.2352 
Epoch 207, current loss (training) 0.2164, current loss (val) 0.2732, best loss (val) 0.2352 
Epoch 208, current loss (training) 0.1867, current loss (val) 0.2508, best loss (val) 0.2352 
Epoch 209, current loss (training) 0.3901, current loss (val) 0.2642, best loss (val) 0.2352 
Epoch 210, current loss (training) 0.1820, current loss (val) 0.2872, best loss (val) 0.2352 
Epoch 211, current loss (training) 0.1730, current loss (val) 0.2808, best loss (val) 0.2352 
Epoch 212, current loss (training) 0.1686, current loss (val) 0.2887, best loss (val) 0.2352 
Epoch 213, current loss (training) 0.1426, current loss (val) 0.2703, best loss (val) 0.2352 
Epoch 214, current loss (training) 0.2077, current loss (val) 0.2843, best loss (val) 0.2352 
Epoch 215, current loss (training) 0.2033, current loss (val) 0.2865, best loss (val) 0.2352 
Epoch 216, current loss (training) 0.2184, current loss (val) 0.3031, best loss (val) 0.2352 
Epoch 217, current loss (training) 0.3905, current loss (val) 0.2657, best loss (val) 0.2352 
Epoch 218, current loss (training) 0.1993, current loss (val) 0.2496, best loss (val) 0.2352 
Epoch 219, current loss (training) 0.1959, current loss (val) 0.2360, best loss (val) 0.2352 
Epoch 220, current loss (training) 0.9403, current loss (val) 0.2437, best loss (val) 0.2352 
Epoch 221, current loss (training) 0.1991, current loss (val) 0.2697, best loss (val) 0.2352 
Epoch 222, current loss (training) 0.9332, current loss (val) 0.2334, best loss (val) 0.2334 
Epoch 223, current loss (training) 0.2667, current loss (val) 0.2954, best loss (val) 0.2334 
Epoch 224, current loss (training) 0.2286, current loss (val) 0.2835, best loss (val) 0.2334 
Epoch 225, current loss (training) 0.1424, current loss (val) 0.2594, best loss (val) 0.2334 
Epoch 226, current loss (training) 0.1696, current loss (val) 0.2609, best loss (val) 0.2334 
Epoch 227, current loss (training) 0.1907, current loss (val) 0.2614, best loss (val) 0.2334 
Epoch 228, current loss (training) 0.2385, current loss (val) 0.3504, best loss (val) 0.2334 
Epoch 229, current loss (training) 0.4473, current loss (val) 0.2769, best loss (val) 0.2334 
Epoch 230, current loss (training) 0.2343, current loss (val) 0.2780, best loss (val) 0.2334 
Epoch 231, current loss (training) 0.1774, current loss (val) 0.2408, best loss (val) 0.2334 
Epoch 232, current loss (training) 0.3231, current loss (val) 0.2767, best loss (val) 0.2334 
Epoch 233, current loss (training) 0.1944, current loss (val) 0.2784, best loss (val) 0.2334 
Epoch 234, current loss (training) 0.2115, current loss (val) 0.2872, best loss (val) 0.2334 
Epoch 235, current loss (training) 0.2408, current loss (val) 0.2908, best loss (val) 0.2334 
Epoch 236, current loss (training) 0.6079, current loss (val) 0.2758, best loss (val) 0.2334 
Epoch 237, current loss (training) 0.3458, current loss (val) 0.2411, best loss (val) 0.2334 
Epoch 238, current loss (training) 0.1708, current loss (val) 0.2700, best loss (val) 0.2334 
Epoch 239, current loss (training) 0.1744, current loss (val) 0.2695, best loss (val) 0.2334 
Epoch 240, current loss (training) 0.1504, current loss (val) 0.2632, best loss (val) 0.2334 
Epoch 241, current loss (training) 0.2102, current loss (val) 0.2554, best loss (val) 0.2334 
Epoch 242, current loss (training) 0.1734, current loss (val) 0.2544, best loss (val) 0.2334 
Epoch 243, current loss (training) 0.1734, current loss (val) 0.2852, best loss (val) 0.2334 
Epoch 244, current loss (training) 0.2091, current loss (val) 0.2825, best loss (val) 0.2334 
Epoch 245, current loss (training) 0.2136, current loss (val) 0.2913, best loss (val) 0.2334 
Epoch 246, current loss (training) 0.3792, current loss (val) 0.2967, best loss (val) 0.2334 
Epoch 247, current loss (training) 0.1763, current loss (val) 0.2752, best loss (val) 0.2334 
Epoch 248, current loss (training) 0.3381, current loss (val) 0.3413, best loss (val) 0.2334 
Epoch 249, current loss (training) 0.1854, current loss (val) 0.2711, best loss (val) 0.2334 
Epoch 250, current loss (training) 0.3579, current loss (val) 0.3134, best loss (val) 0.2334 
Epoch 251, current loss (training) 0.1954, current loss (val) 0.2601, best loss (val) 0.2334 
Epoch 252, current loss (training) 0.3349, current loss (val) 0.2812, best loss (val) 0.2334 
Epoch 253, current loss (training) 0.6450, current loss (val) 0.3159, best loss (val) 0.2334 
Epoch 254, current loss (training) 0.2874, current loss (val) 0.2556, best loss (val) 0.2334 
Epoch 255, current loss (training) 0.2499, current loss (val) 0.2945, best loss (val) 0.2334 
Epoch 256, current loss (training) 0.2561, current loss (val) 0.2811, best loss (val) 0.2334 
Epoch 257, current loss (training) 0.4482, current loss (val) 0.3256, best loss (val) 0.2334 
Epoch 258, current loss (training) 0.5767, current loss (val) 0.2598, best loss (val) 0.2334 
Epoch 259, current loss (training) 0.1811, current loss (val) 0.2505, best loss (val) 0.2334 
Epoch 260, current loss (training) 0.1479, current loss (val) 0.2501, best loss (val) 0.2334 
Epoch 261, current loss (training) 0.6252, current loss (val) 0.2977, best loss (val) 0.2334 
Epoch 262, current loss (training) 0.2640, current loss (val) 0.2544, best loss (val) 0.2334 
Epoch 263, current loss (training) 0.2498, current loss (val) 0.2527, best loss (val) 0.2334 
Epoch 264, current loss (training) 0.1798, current loss (val) 0.2649, best loss (val) 0.2334 
Epoch 265, current loss (training) 0.1801, current loss (val) 0.2824, best loss (val) 0.2334 
Epoch 266, current loss (training) 0.5946, current loss (val) 0.2618, best loss (val) 0.2334 
Epoch 267, current loss (training) 1.3521, current loss (val) 0.3323, best loss (val) 0.2334 
Epoch 268, current loss (training) 0.3842, current loss (val) 0.2831, best loss (val) 0.2334 
Epoch 269, current loss (training) 1.5799, current loss (val) 1.6849, best loss (val) 0.2334 
Epoch 270, current loss (training) 0.1948, current loss (val) 0.2646, best loss (val) 0.2334 
Epoch 271, current loss (training) 0.2239, current loss (val) 0.2864, best loss (val) 0.2334 
Epoch 272, current loss (training) 0.1939, current loss (val) 0.3058, best loss (val) 0.2334 
Epoch 273, current loss (training) 0.2763, current loss (val) 0.2458, best loss (val) 0.2334 
Epoch 274, current loss (training) 0.2575, current loss (val) 0.2659, best loss (val) 0.2334 
Epoch 275, current loss (training) 0.1939, current loss (val) 0.3093, best loss (val) 0.2334 
Epoch 276, current loss (training) 0.1508, current loss (val) 0.2965, best loss (val) 0.2334 
Epoch 277, current loss (training) 0.1322, current loss (val) 0.2586, best loss (val) 0.2334 
Epoch 278, current loss (training) 0.4010, current loss (val) 0.2652, best loss (val) 0.2334 
Epoch 279, current loss (training) 0.2156, current loss (val) 0.3012, best loss (val) 0.2334 
Epoch 280, current loss (training) 0.2862, current loss (val) 0.2790, best loss (val) 0.2334 
Epoch 281, current loss (training) 0.1814, current loss (val) 0.2790, best loss (val) 0.2334 
Epoch 282, current loss (training) 0.5879, current loss (val) 0.2965, best loss (val) 0.2334 
Epoch 283, current loss (training) 0.3348, current loss (val) 0.4090, best loss (val) 0.2334 
Epoch 284, current loss (training) 0.7911, current loss (val) 0.2904, best loss (val) 0.2334 
Epoch 285, current loss (training) 0.9063, current loss (val) 0.2872, best loss (val) 0.2334 
Epoch 286, current loss (training) 0.1409, current loss (val) 0.2945, best loss (val) 0.2334 
Epoch 287, current loss (training) 0.3381, current loss (val) 0.2817, best loss (val) 0.2334 
Epoch 288, current loss (training) 0.1830, current loss (val) 0.2982, best loss (val) 0.2334 
Epoch 289, current loss (training) 0.1895, current loss (val) 0.2544, best loss (val) 0.2334 
Epoch 290, current loss (training) 0.2522, current loss (val) 0.2860, best loss (val) 0.2334 
Epoch 291, current loss (training) 0.3041, current loss (val) 0.2957, best loss (val) 0.2334 
Epoch 292, current loss (training) 0.1518, current loss (val) 0.2486, best loss (val) 0.2334 
Epoch 293, current loss (training) 0.1922, current loss (val) 0.2869, best loss (val) 0.2334 
Epoch 294, current loss (training) 0.2072, current loss (val) 0.2713, best loss (val) 0.2334 
Epoch 295, current loss (training) 0.2109, current loss (val) 0.2728, best loss (val) 0.2334 
Epoch 296, current loss (training) 0.2616, current loss (val) 0.2839, best loss (val) 0.2334 
Epoch 297, current loss (training) 0.2474, current loss (val) 0.3223, best loss (val) 0.2334 
Epoch 298, current loss (training) 0.1809, current loss (val) 0.2492, best loss (val) 0.2334 
Epoch 299, current loss (training) 0.1710, current loss (val) 0.3102, best loss (val) 0.2334 
Epoch 300, current loss (training) 0.1736, current loss (val) 0.2700, best loss (val) 0.2334 
Epoch 301, current loss (training) 0.1854, current loss (val) 0.2487, best loss (val) 0.2334 
Epoch 302, current loss (training) 0.2131, current loss (val) 0.2832, best loss (val) 0.2334 
Epoch 303, current loss (training) 0.2335, current loss (val) 0.3161, best loss (val) 0.2334 
Epoch 304, current loss (training) 0.1869, current loss (val) 0.2581, best loss (val) 0.2334 
Epoch 305, current loss (training) 0.1588, current loss (val) 0.2637, best loss (val) 0.2334 
Epoch 306, current loss (training) 0.1700, current loss (val) 0.2566, best loss (val) 0.2334 
Epoch 307, current loss (training) 0.5218, current loss (val) 0.5574, best loss (val) 0.2334 
Epoch 308, current loss (training) 0.1556, current loss (val) 0.2766, best loss (val) 0.2334 
Epoch 309, current loss (training) 0.2475, current loss (val) 0.2861, best loss (val) 0.2334 
Epoch 310, current loss (training) 0.9021, current loss (val) 0.5168, best loss (val) 0.2334 
Epoch 311, current loss (training) 0.1855, current loss (val) 0.2630, best loss (val) 0.2334 
Epoch 312, current loss (training) 0.2342, current loss (val) 0.2450, best loss (val) 0.2334 
Epoch 313, current loss (training) 0.5407, current loss (val) 0.2616, best loss (val) 0.2334 
Epoch 314, current loss (training) 0.1965, current loss (val) 0.2496, best loss (val) 0.2334 
Epoch 315, current loss (training) 0.1721, current loss (val) 0.2763, best loss (val) 0.2334 
Epoch 316, current loss (training) 0.2206, current loss (val) 0.3101, best loss (val) 0.2334 
Epoch 317, current loss (training) 0.4678, current loss (val) 0.2768, best loss (val) 0.2334 
Epoch 318, current loss (training) 0.2646, current loss (val) 0.2989, best loss (val) 0.2334 
Epoch 319, current loss (training) 0.3353, current loss (val) 0.3635, best loss (val) 0.2334 
Epoch 320, current loss (training) 0.2501, current loss (val) 0.2587, best loss (val) 0.2334 
Epoch 321, current loss (training) 0.1882, current loss (val) 0.2638, best loss (val) 0.2334 
Epoch 322, current loss (training) 0.4027, current loss (val) 0.2941, best loss (val) 0.2334 
Epoch 323, current loss (training) 0.1847, current loss (val) 0.3089, best loss (val) 0.2334 
Epoch 324, current loss (training) 0.1519, current loss (val) 0.2838, best loss (val) 0.2334 
Epoch 325, current loss (training) 0.3775, current loss (val) 0.3836, best loss (val) 0.2334 
Epoch 326, current loss (training) 0.1676, current loss (val) 0.3210, best loss (val) 0.2334 
Epoch 327, current loss (training) 0.3902, current loss (val) 0.2845, best loss (val) 0.2334 
Epoch 328, current loss (training) 0.2099, current loss (val) 0.2774, best loss (val) 0.2334 
Epoch 329, current loss (training) 0.2560, current loss (val) 0.2792, best loss (val) 0.2334 
Epoch 330, current loss (training) 0.1604, current loss (val) 0.2626, best loss (val) 0.2334 
Epoch 331, current loss (training) 0.6338, current loss (val) 0.2509, best loss (val) 0.2334 
Epoch 332, current loss (training) 0.3771, current loss (val) 0.2682, best loss (val) 0.2334 
Epoch 333, current loss (training) 0.1595, current loss (val) 0.2707, best loss (val) 0.2334 
Epoch 334, current loss (training) 0.2972, current loss (val) 0.2744, best loss (val) 0.2334 
Epoch 335, current loss (training) 0.1441, current loss (val) 0.2397, best loss (val) 0.2334 
Epoch 336, current loss (training) 0.4821, current loss (val) 0.2777, best loss (val) 0.2334 
Epoch 337, current loss (training) 0.2866, current loss (val) 0.2519, best loss (val) 0.2334 
Epoch 338, current loss (training) 0.2502, current loss (val) 0.2550, best loss (val) 0.2334 
Epoch 339, current loss (training) 0.3299, current loss (val) 0.2511, best loss (val) 0.2334 
Epoch 340, current loss (training) 0.2523, current loss (val) 0.2566, best loss (val) 0.2334 
Epoch 341, current loss (training) 0.3156, current loss (val) 0.2937, best loss (val) 0.2334 
Epoch 342, current loss (training) 0.1689, current loss (val) 0.2647, best loss (val) 0.2334 
Epoch 343, current loss (training) 0.2031, current loss (val) 0.2777, best loss (val) 0.2334 
Epoch 344, current loss (training) 0.2356, current loss (val) 0.2795, best loss (val) 0.2334 
Epoch 345, current loss (training) 0.1706, current loss (val) 0.2637, best loss (val) 0.2334 
Epoch 346, current loss (training) 0.4995, current loss (val) 0.2911, best loss (val) 0.2334 
Epoch 347, current loss (training) 0.2411, current loss (val) 0.2671, best loss (val) 0.2334 
Epoch 348, current loss (training) 0.3092, current loss (val) 0.2707, best loss (val) 0.2334 
Epoch 349, current loss (training) 0.1564, current loss (val) 0.2642, best loss (val) 0.2334 
Epoch 350, current loss (training) 0.1744, current loss (val) 0.2773, best loss (val) 0.2334 
Epoch 351, current loss (training) 0.1502, current loss (val) 0.2585, best loss (val) 0.2334 
Epoch 352, current loss (training) 0.1539, current loss (val) 0.2476, best loss (val) 0.2334 
Epoch 353, current loss (training) 0.1630, current loss (val) 0.2535, best loss (val) 0.2334 
Epoch 354, current loss (training) 0.2023, current loss (val) 0.2808, best loss (val) 0.2334 
Epoch 355, current loss (training) 0.2163, current loss (val) 0.2405, best loss (val) 0.2334 
Epoch 356, current loss (training) 0.1924, current loss (val) 0.2680, best loss (val) 0.2334 
Epoch 357, current loss (training) 0.2196, current loss (val) 0.2898, best loss (val) 0.2334 
Epoch 358, current loss (training) 0.2046, current loss (val) 0.2509, best loss (val) 0.2334 
Epoch 359, current loss (training) 0.2647, current loss (val) 0.2693, best loss (val) 0.2334 
Epoch 360, current loss (training) 0.3624, current loss (val) 0.2486, best loss (val) 0.2334 
Epoch 361, current loss (training) 0.1771, current loss (val) 0.2852, best loss (val) 0.2334 
Epoch 362, current loss (training) 0.2032, current loss (val) 0.2721, best loss (val) 0.2334 
Epoch 363, current loss (training) 0.2833, current loss (val) 0.2821, best loss (val) 0.2334 
Epoch 364, current loss (training) 0.1821, current loss (val) 0.2780, best loss (val) 0.2334 
Epoch 365, current loss (training) 0.1350, current loss (val) 0.2431, best loss (val) 0.2334 
Epoch 366, current loss (training) 0.1952, current loss (val) 0.2834, best loss (val) 0.2334 
Epoch 367, current loss (training) 0.2188, current loss (val) 0.2658, best loss (val) 0.2334 
Epoch 368, current loss (training) 0.2024, current loss (val) 0.2598, best loss (val) 0.2334 
Epoch 369, current loss (training) 0.3489, current loss (val) 0.2687, best loss (val) 0.2334 
Epoch 370, current loss (training) 0.2266, current loss (val) 0.2867, best loss (val) 0.2334 
Epoch 371, current loss (training) 0.1767, current loss (val) 0.2592, best loss (val) 0.2334 
Epoch 372, current loss (training) 0.1631, current loss (val) 0.2602, best loss (val) 0.2334 
Epoch 373, current loss (training) 0.2867, current loss (val) 0.2459, best loss (val) 0.2334 
Epoch 374, current loss (training) 0.3183, current loss (val) 0.2548, best loss (val) 0.2334 
Epoch 375, current loss (training) 0.3321, current loss (val) 0.2660, best loss (val) 0.2334 
Epoch 376, current loss (training) 0.1721, current loss (val) 0.2327, best loss (val) 0.2327 
Epoch 377, current loss (training) 0.2245, current loss (val) 0.3194, best loss (val) 0.2327 
Epoch 378, current loss (training) 0.2035, current loss (val) 0.3045, best loss (val) 0.2327 
Epoch 379, current loss (training) 0.2342, current loss (val) 0.2653, best loss (val) 0.2327 
Epoch 380, current loss (training) 0.1514, current loss (val) 0.2625, best loss (val) 0.2327 
Epoch 381, current loss (training) 0.1623, current loss (val) 0.2728, best loss (val) 0.2327 
Epoch 382, current loss (training) 0.3245, current loss (val) 0.2936, best loss (val) 0.2327 
Epoch 383, current loss (training) 0.1840, current loss (val) 0.3286, best loss (val) 0.2327 
Epoch 384, current loss (training) 0.3210, current loss (val) 0.3384, best loss (val) 0.2327 
Epoch 385, current loss (training) 0.2368, current loss (val) 0.2667, best loss (val) 0.2327 
Epoch 386, current loss (training) 0.3384, current loss (val) 0.2893, best loss (val) 0.2327 
Epoch 387, current loss (training) 0.2507, current loss (val) 0.2517, best loss (val) 0.2327 
Epoch 388, current loss (training) 0.1629, current loss (val) 0.3031, best loss (val) 0.2327 
Epoch 389, current loss (training) 0.2977, current loss (val) 0.3652, best loss (val) 0.2327 
Epoch 390, current loss (training) 0.2750, current loss (val) 0.2766, best loss (val) 0.2327 
Epoch 391, current loss (training) 0.1884, current loss (val) 0.2594, best loss (val) 0.2327 
Epoch 392, current loss (training) 0.2073, current loss (val) 0.2319, best loss (val) 0.2319 
Epoch 393, current loss (training) 0.2390, current loss (val) 0.2617, best loss (val) 0.2319 
Epoch 394, current loss (training) 0.2092, current loss (val) 0.2708, best loss (val) 0.2319 
Epoch 395, current loss (training) 0.1693, current loss (val) 0.3054, best loss (val) 0.2319 
Epoch 396, current loss (training) 0.3643, current loss (val) 0.2792, best loss (val) 0.2319 
Epoch 397, current loss (training) 0.2373, current loss (val) 0.2971, best loss (val) 0.2319 
Epoch 398, current loss (training) 0.1984, current loss (val) 0.2782, best loss (val) 0.2319 
Epoch 399, current loss (training) 0.1697, current loss (val) 0.2479, best loss (val) 0.2319 
Epoch 400, current loss (training) 0.2310, current loss (val) 0.3079, best loss (val) 0.2319 
Epoch 401, current loss (training) 0.1696, current loss (val) 0.2557, best loss (val) 0.2319 
Epoch 402, current loss (training) 0.2101, current loss (val) 0.2943, best loss (val) 0.2319 
Epoch 403, current loss (training) 0.1869, current loss (val) 0.2890, best loss (val) 0.2319 
Epoch 404, current loss (training) 0.4167, current loss (val) 0.2586, best loss (val) 0.2319 
Epoch 405, current loss (training) 0.2363, current loss (val) 0.2754, best loss (val) 0.2319 
Epoch 406, current loss (training) 0.2017, current loss (val) 0.2498, best loss (val) 0.2319 
Epoch 407, current loss (training) 0.1456, current loss (val) 0.2565, best loss (val) 0.2319 
Epoch 408, current loss (training) 1.1423, current loss (val) 0.2629, best loss (val) 0.2319 
Epoch 409, current loss (training) 0.3196, current loss (val) 0.2602, best loss (val) 0.2319 
Epoch 410, current loss (training) 0.3768, current loss (val) 0.2738, best loss (val) 0.2319 
Epoch 411, current loss (training) 0.1589, current loss (val) 0.3061, best loss (val) 0.2319 
Epoch 412, current loss (training) 0.2039, current loss (val) 0.2559, best loss (val) 0.2319 
Epoch 413, current loss (training) 0.1519, current loss (val) 0.2695, best loss (val) 0.2319 
Epoch 414, current loss (training) 0.2105, current loss (val) 0.2572, best loss (val) 0.2319 
Epoch 415, current loss (training) 0.7342, current loss (val) 0.2633, best loss (val) 0.2319 
Epoch 416, current loss (training) 0.1511, current loss (val) 0.2525, best loss (val) 0.2319 
Epoch 417, current loss (training) 0.1532, current loss (val) 0.2442, best loss (val) 0.2319 
Epoch 418, current loss (training) 0.1819, current loss (val) 0.2678, best loss (val) 0.2319 
Epoch 419, current loss (training) 0.1972, current loss (val) 0.2467, best loss (val) 0.2319 
Epoch 420, current loss (training) 0.1868, current loss (val) 0.2603, best loss (val) 0.2319 
Epoch 421, current loss (training) 0.2239, current loss (val) 0.2794, best loss (val) 0.2319 
Epoch 422, current loss (training) 0.2304, current loss (val) 0.3634, best loss (val) 0.2319 
Epoch 423, current loss (training) 0.2517, current loss (val) 0.2569, best loss (val) 0.2319 
Epoch 424, current loss (training) 0.3071, current loss (val) 0.2977, best loss (val) 0.2319 
Epoch 425, current loss (training) 0.2733, current loss (val) 0.2895, best loss (val) 0.2319 
Epoch 426, current loss (training) 0.2318, current loss (val) 0.2498, best loss (val) 0.2319 
Epoch 427, current loss (training) 0.2850, current loss (val) 0.2648, best loss (val) 0.2319 
Epoch 428, current loss (training) 0.2935, current loss (val) 0.2924, best loss (val) 0.2319 
Epoch 429, current loss (training) 0.1652, current loss (val) 0.2468, best loss (val) 0.2319 
Epoch 430, current loss (training) 0.1912, current loss (val) 0.2659, best loss (val) 0.2319 
Epoch 431, current loss (training) 0.1560, current loss (val) 0.2609, best loss (val) 0.2319 
Epoch 432, current loss (training) 0.1964, current loss (val) 0.2610, best loss (val) 0.2319 
Epoch 433, current loss (training) 0.4318, current loss (val) 0.2792, best loss (val) 0.2319 
Epoch 434, current loss (training) 0.2984, current loss (val) 0.3528, best loss (val) 0.2319 
Epoch 435, current loss (training) 0.1732, current loss (val) 0.2745, best loss (val) 0.2319 
Epoch 436, current loss (training) 0.1972, current loss (val) 0.2674, best loss (val) 0.2319 
Epoch 437, current loss (training) 0.1692, current loss (val) 0.2661, best loss (val) 0.2319 
Epoch 438, current loss (training) 0.2666, current loss (val) 0.2514, best loss (val) 0.2319 
Epoch 439, current loss (training) 0.2305, current loss (val) 0.2623, best loss (val) 0.2319 
Epoch 440, current loss (training) 0.8158, current loss (val) 0.2427, best loss (val) 0.2319 
Epoch 441, current loss (training) 0.2336, current loss (val) 0.2775, best loss (val) 0.2319 
Epoch 442, current loss (training) 0.6455, current loss (val) 0.2535, best loss (val) 0.2319 
Epoch 443, current loss (training) 0.4269, current loss (val) 0.2706, best loss (val) 0.2319 
Epoch 444, current loss (training) 0.2242, current loss (val) 0.2572, best loss (val) 0.2319 
Epoch 445, current loss (training) 0.3384, current loss (val) 0.2559, best loss (val) 0.2319 
Epoch 446, current loss (training) 0.1917, current loss (val) 0.2607, best loss (val) 0.2319 
Epoch 447, current loss (training) 0.2411, current loss (val) 0.3273, best loss (val) 0.2319 
Epoch 448, current loss (training) 0.2485, current loss (val) 0.2673, best loss (val) 0.2319 
Epoch 449, current loss (training) 0.2227, current loss (val) 0.2736, best loss (val) 0.2319 
Epoch 450, current loss (training) 0.1819, current loss (val) 0.2717, best loss (val) 0.2319 
Epoch 451, current loss (training) 0.2036, current loss (val) 0.2714, best loss (val) 0.2319 
Epoch 452, current loss (training) 0.2922, current loss (val) 0.3173, best loss (val) 0.2319 
Epoch 453, current loss (training) 0.2343, current loss (val) 0.2772, best loss (val) 0.2319 
Epoch 454, current loss (training) 0.1669, current loss (val) 0.2438, best loss (val) 0.2319 
Epoch 455, current loss (training) 0.1928, current loss (val) 0.2731, best loss (val) 0.2319 
Epoch 456, current loss (training) 0.2737, current loss (val) 0.2716, best loss (val) 0.2319 
Epoch 457, current loss (training) 0.3199, current loss (val) 0.2782, best loss (val) 0.2319 
Epoch 458, current loss (training) 0.4138, current loss (val) 0.2882, best loss (val) 0.2319 
Epoch 459, current loss (training) 0.1932, current loss (val) 0.2656, best loss (val) 0.2319 
Epoch 460, current loss (training) 0.2387, current loss (val) 0.2791, best loss (val) 0.2319 
Epoch 461, current loss (training) 0.2565, current loss (val) 0.2700, best loss (val) 0.2319 
Epoch 462, current loss (training) 0.1682, current loss (val) 0.2939, best loss (val) 0.2319 
Epoch 463, current loss (training) 0.1488, current loss (val) 0.2555, best loss (val) 0.2319 
Epoch 464, current loss (training) 0.1371, current loss (val) 0.2754, best loss (val) 0.2319 
Epoch 465, current loss (training) 0.1626, current loss (val) 0.2741, best loss (val) 0.2319 
Epoch 466, current loss (training) 0.1709, current loss (val) 0.2543, best loss (val) 0.2319 
Epoch 467, current loss (training) 0.3387, current loss (val) 0.2535, best loss (val) 0.2319 
Epoch 468, current loss (training) 0.2155, current loss (val) 0.2669, best loss (val) 0.2319 
Epoch 469, current loss (training) 0.1704, current loss (val) 0.2434, best loss (val) 0.2319 
Epoch 470, current loss (training) 0.2485, current loss (val) 0.2959, best loss (val) 0.2319 
Epoch 471, current loss (training) 0.1918, current loss (val) 0.2740, best loss (val) 0.2319 
Epoch 472, current loss (training) 0.2112, current loss (val) 0.2683, best loss (val) 0.2319 
Epoch 473, current loss (training) 0.2145, current loss (val) 0.2589, best loss (val) 0.2319 
Epoch 474, current loss (training) 0.2954, current loss (val) 0.3516, best loss (val) 0.2319 
Epoch 475, current loss (training) 0.2109, current loss (val) 0.2534, best loss (val) 0.2319 
Epoch 476, current loss (training) 0.1596, current loss (val) 0.2870, best loss (val) 0.2319 
Epoch 477, current loss (training) 0.1856, current loss (val) 0.2852, best loss (val) 0.2319 
Epoch 478, current loss (training) 0.1844, current loss (val) 0.2676, best loss (val) 0.2319 
Epoch 479, current loss (training) 0.2674, current loss (val) 0.2976, best loss (val) 0.2319 
Epoch 480, current loss (training) 0.1581, current loss (val) 0.2574, best loss (val) 0.2319 
Epoch 481, current loss (training) 0.2168, current loss (val) 0.3712, best loss (val) 0.2319 
Epoch 482, current loss (training) 0.3110, current loss (val) 0.2612, best loss (val) 0.2319 
Epoch 483, current loss (training) 0.6007, current loss (val) 0.2929, best loss (val) 0.2319 
Epoch 484, current loss (training) 0.1568, current loss (val) 0.2493, best loss (val) 0.2319 
Epoch 485, current loss (training) 0.1528, current loss (val) 0.3035, best loss (val) 0.2319 
Epoch 486, current loss (training) 0.3739, current loss (val) 0.2826, best loss (val) 0.2319 
Epoch 487, current loss (training) 0.1797, current loss (val) 0.2569, best loss (val) 0.2319 
Epoch 488, current loss (training) 0.3871, current loss (val) 0.3231, best loss (val) 0.2319 
Epoch 489, current loss (training) 0.1835, current loss (val) 0.2646, best loss (val) 0.2319 
Epoch 490, current loss (training) 0.2624, current loss (val) 0.2763, best loss (val) 0.2319 
Epoch 491, current loss (training) 0.2005, current loss (val) 0.2709, best loss (val) 0.2319 
Epoch 492, current loss (training) 0.2617, current loss (val) 0.2678, best loss (val) 0.2319 
Epoch 493, current loss (training) 0.1859, current loss (val) 0.2867, best loss (val) 0.2319 
Epoch 494, current loss (training) 0.1410, current loss (val) 0.2731, best loss (val) 0.2319 
Epoch 495, current loss (training) 0.1910, current loss (val) 0.2781, best loss (val) 0.2319 
Epoch 496, current loss (training) 0.1951, current loss (val) 0.2908, best loss (val) 0.2319 
Epoch 497, current loss (training) 0.1767, current loss (val) 0.2889, best loss (val) 0.2319 
Epoch 498, current loss (training) 0.2002, current loss (val) 0.2799, best loss (val) 0.2319 
Epoch 499, current loss (training) 0.1409, current loss (val) 0.2716, best loss (val) 0.2319 
Epoch 500, current loss (training) 0.1946, current loss (val) 0.2607, best loss (val) 0.2319 
run_time_first_training_cycle:  78948.80 

Epochs 500, batch size, 200
Nbr training obs 500000, nbr parameters 22214, obs/parameters 22.51

Starting abc-rs
Percentage done:  2.00 %
Percentage done:  4.00 %
Percentage done:  6.00 %
Percentage done:  8.00 %
Percentage done:  10.00 %
Percentage done:  12.00 %
Percentage done:  14.00 %
Percentage done:  16.00 %
Percentage done:  18.00 %
Percentage done:  20.00 %
Percentage done:  22.00 %
Percentage done:  24.00 %
Percentage done:  26.00 %
Percentage done:  28.00 %
Percentage done:  30.00 %
Percentage done:  32.00 %
Percentage done:  34.00 %
Percentage done:  36.00 %
Percentage done:  38.00 %
Percentage done:  40.00 %
Percentage done:  42.00 %
Percentage done:  44.00 %
Percentage done:  46.00 %
Percentage done:  48.00 %
Percentage done:  50.00 %
Percentage done:  52.00 %
Percentage done:  54.00 %
Percentage done:  56.00 %
Percentage done:  58.00 %
Percentage done:  60.00 %
Percentage done:  62.00 %
Percentage done:  64.00 %
Percentage done:  66.00 %
Percentage done:  68.00 %
Percentage done:  70.00 %
Percentage done:  72.00 %
Percentage done:  74.00 %
Percentage done:  76.00 %
Percentage done:  78.00 %
Percentage done:  80.00 %
Percentage done:  82.00 %
Percentage done:  84.00 %
Percentage done:  86.00 %
Percentage done:  88.00 %
Percentage done:  90.00 %
Percentage done:  92.00 %
Percentage done:  94.00 %
Percentage done:  96.00 %
Percentage done:  98.00 %
Percentage done:  100.00 %
Ending abc-rs
end script
